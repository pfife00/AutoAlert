# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

from __future__ import print_function

import sys
from pyspark.streaming.kafka import KafkaUtils, TopicAndPartition
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import Row, SparkSession


def getSparkSessionInstance(sparkConf):
    if ('sparkSessionSingletonInstance' not in globals()):
        globals()['sparkSessionSingletonInstance'] = SparkSession\
            .builder\
            .config(conf=sparkConf)\
            .getOrCreate()
    return globals()['sparkSessionSingletonInstance']


if __name__ == "__main__":
    #if len(sys.argv) != 3:
    #    print("Usage: sql_network_wordcount.py <hostname> <port> ", file=sys.stderr)
    #    exit(-1)
    #host, port = sys.argv[1:]
    sc = SparkContext(appName="PythonSqlNetworkWordCount")
    ssc = StreamingContext(sc, 1)
    #topic = 'rawDBGData'
    #partition = 0
    #start = 0
    #topicpartition = TopicAndPartition(topic, partition)
    #fromoffset = {topicpartition: int(start)}
    #parse the row into separate components

    #kafkaStream = KafkaUtils.createDirectStream(ssc,
    #                ['rawDBGData'], {'metadata.broker.list':
    #                '10.0.0.11:9092, 10.0.0.9:9092, 10.0.0.4:9092'}, fromOffsets = fromoffset)

    kafkaStream = KafkaUtils.createDirectStream(ssc,
                    ['rawDBGData'], {'metadata.broker.list':
                    '10.0.0.11:9092, 10.0.0.9:9092, 10.0.0.4:9092'})

    #parse the row into separate components
    filteredStream = kafkaStream.map(lambda line: line[1].split("^")).pprint()
        # Convert RDDs of the words DStream to DataFrame and run SQL query


    ssc.start()
    ssc.awaitTermination()
