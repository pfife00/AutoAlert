{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 0 - Configure Cluster System </h3>\n",
    "    <ul>\n",
    "    <li>First setup a 4 node cluster called spark-cluster using 4 m4.large ec2 instances.</li>\n",
    "    <li>Next configure each node following instructions to setup and configure java and passwordless SSH between each node</li>\n",
    "</ul>\n",
    "<h3>Step 1 - Install Scala on each node </h3>\n",
    "<code>sudo apt install scala</code><br>\n",
    "<code>scala -version</code>\n",
    "Make node of scala version:\n",
    "Scala code runner version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL\n",
    "<h3>Step 2: Installing Spark</h3>\n",
    "<ul>\n",
    "<li>On each machine (both master and worker) install Spark using the following commands. You can configure your version by visiting https://spark.apache.org/downloads.html<br>\n",
    "<code>wget http://apache.claz.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz</code><br></li>\n",
    "<li>Extract the files and move them to /usr/local/spark and add the spark/bin into PATH variable.</li>\n",
    "<code>tar xvf spark-2.4.3-bin-hadoop2.7.tgz</code><br>\n",
    "<code>sudo mv spark-2.4.3-bin-hadoop2.7/ /usr/local/spark</code><br>\n",
    "<code>vi ~/.bash_profile</code><br>\n",
    "export PATH=/usr/local/spark/bin:$PATH<br>\n",
    "<code>source ~/.bash_profile </code>\n",
    "</ul>\n",
    "<h3>Step 3: Configuring Master to keep track of its workers</h3>\n",
    "<ul>\n",
    "<li>Modify /usr/local/spark/conf/spark-env.sh file by providing information about Java location and the master node’s IP.<br>\n",
    "contents of conf/spark-env.sh</li>\n",
    "<code>export SPARK_MASTER_HOST=master-private-ip</code><br>\n",
    "<code>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/</code><br>\n",
    "Note: If the spark-env.sh doesn’t exist copy the spark-env.sh.template and rename it.<br>\n",
    "\n",
    "<li>Add all the IPs where the worker will be started. Open the /usr/local/spark/conf/slaves file and paste the following.<br>\n",
    "contents of conf/slaves\n",
    "<li>worker1 private IP</li>\n",
    "<li>worker2 private IP</li>\n",
    "<li>worker3 private IP</li></li>\n",
    "</ul>\n",
    "\n",
    "<h3>Step 4: Set oversubscription</h3>\n",
    "Set the SPARK_WORKER_CORES flag in spark-env.sh to a value higher than the actual number of cores.\n",
    "Example: If each of our three workers have 2 cores, we have total of 6 cores available. We can set the WORKER_CORES to be 3 times that to allow for oversubscription.<br>\n",
    "<code>export SPARK_WORKER_CORES=6</code>\n",
    "\n",
    "<h3>Step 5: Test Configuration</h3>\n",
    "Run script and it starts it as a background process so you can exit the terminal.\n",
    "<code>sh /usr/local/spark/sbin/start-all.sh</code><br>\n",
    "Once you want to stop the service you can run<br>\n",
    "<code>sbin/stop-all.sh</code>\n",
    "\n",
    "<h3>Can use Pegasus to Install Spark as well</h3>\n",
    "Follow steps in Pegasus documentation to install Spark with Pegasus if prefer that route\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
