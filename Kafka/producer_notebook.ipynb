{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KafkaTimeoutError",
     "evalue": "KafkaTimeoutError: Failed to update metadata after 60.0 secs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKafkaTimeoutError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-78cd4821b2b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-78cd4821b2b8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"^\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                                 \u001b[0;31m#print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                                 \u001b[0mproducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rawDBGData'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8 '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                                 \u001b[0;31m#make the client wait for any outstanding messages to be delivered to the broker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                 \u001b[0;31m#producer.flush()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/kafka/producer/kafka.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, topic, value, key, headers, partition, timestamp_ms)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mkey_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_on_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_block_ms'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             key_bytes = self._serialize(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/kafka/producer/kafka.py\u001b[0m in \u001b[0;36m_wait_on_metadata\u001b[0;34m(self, topic, max_wait)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmetadata_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                 raise Errors.KafkaTimeoutError(\n\u001b[0;32m--> 691\u001b[0;31m                     \"Failed to update metadata after %.1f secs.\" % (max_wait,))\n\u001b[0m\u001b[1;32m    692\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munauthorized_topics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTopicAuthorizationFailedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKafkaTimeoutError\u001b[0m: KafkaTimeoutError: Failed to update metadata after 60.0 secs."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "############################################################\n",
    "# Author: Forest Pfeiffer\n",
    "#Based on producer.py written by Ryan Hebel during Insight DE\n",
    "#2018C session\n",
    "\n",
    "#Create a Kafka producer\n",
    "# that reads data from a S3 bucket and sends it to a topic.\n",
    "############################################################\n",
    "\n",
    "#to install boto3, run the following command:\n",
    "#pip install boto3\n",
    "import configparser\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "        kafka_nodes = 'ec2-50-112-13-159.us-west-2.compute.amazonaws.com:9092'\n",
    "        #create Kafka producer that communicates with master node of ec2 instance running Kafka\n",
    "        producer = KafkaProducer(bootstrap_servers = [kafka_nodes], api_version=(0,10,2,0))\n",
    "\n",
    "        #read credentials from dwg.cfg file (note this file will not be stored\n",
    "        #on github for security reasons)\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('dwh.cfg')\n",
    "        AWS_ACCESS_KEY_ID = config.get('AWS', 'KEY')\n",
    "        AWS_SECRET_ACCESS_KEY = config.get('AWS', 'SECRET')\n",
    "\n",
    "        #creates bucket that points to data\n",
    "        s3 = boto3.resource('s3', aws_access_key_id = AWS_ACCESS_KEY_ID, aws_secret_access_key = AWS_SECRET_ACCESS_KEY)\n",
    "        bucket = s3.Bucket('deutsche-boerse-xetra-pds')\n",
    "        #i=0\n",
    "        #the deutsche-boerse-xetra-pds bucket contains a list of csv links that\n",
    "        #point to data for each hour of trading\n",
    "        #first, iterate through each object (link to csv) in the bucket\n",
    "        for object in bucket.objects.all():\n",
    "                #i+=1\n",
    "                #if i == 200:\n",
    "                #        break\n",
    "                #filter for non-trading hours (empty csv) by size\n",
    "                #files with size 136 bytes indicate off hours logging and\n",
    "                #should be ommitted\n",
    "                if object.size > 136:\n",
    "                        url = 'https://s3.eu-central-1.amazonaws.com/deutsche-boerse-xetra-pds/' + object.key\n",
    "                       \n",
    "                        data = pd.read_csv(url)\n",
    "                        #read through each line of csv and send the line to the kafka topic\n",
    "                        #files with size 136 bytes indicate off hours logging and\n",
    "                        #should be ommitted\n",
    "                        for index, row in data.iterrows():\n",
    "                                output = ''\n",
    "                                for element in row:\n",
    "                                        output = output + str(element) + \"^\"\n",
    "                                #print(output)\n",
    "                                producer.send('rawDBGData', output.encode('utf8 '))\n",
    "                                #make the client wait for any outstanding messages to be delivered to the broker\n",
    "                                #producer.flush()\n",
    "        #producer.close()\n",
    "        return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
